{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ntVaQWp2H4Y"
   },
   "source": [
    "# メモ\n",
    "- 2GPU以上での実行をお願いします。\n",
    "- また途中でメモリ解放のためのカーネル再起動があります。\n",
    "- GPTQモデルロード時に「CUDA extension not installed.」が表示されることがありますが、  \n",
    "下記実行結果の通り、以降のセル実行に影響はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-xAlbvwfg8s"
   },
   "source": [
    "## 環境設定\n",
    "## ライブラリのインストール(omnicampus上の環境にないライブラリをインストールした方のみ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OFgvD7RM2S0d"
   },
   "outputs": [],
   "source": [
    "import sys, os, warnings, gc, time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcsvmBjCj7Na",
    "outputId": "031a1843-8a85-4401-d30f-de849dc83d11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 13 00:37:11 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   61C    P8              14W /  72W |     87MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   55C    P8              13W /  72W |     17MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#演習環境\n",
    "# GPUの確認、枚数に応じaて割り当てられた計算資源の消費速度が異なるのでお気をつけください。コンペ期間中は2時間で自動でインスタンスが落ちず、無制限となるため、インスタンスの停止し忘れにもご注意ください。\n",
    "# 50GPU時間を超えると、インスタンスを立ち上げることができませんのでご注意ください。\n",
    "!nvidia-smi\n",
    "\n",
    "# /workspace/assets以下のファイルは永続化されます。作業中のcodeや学習した重みやデータはこちらに保存してください。(容量数TB、IOは遅い)\n",
    "# /workspace/assets以外の部分のデータ容量は100GBまでです。複数モデルをダウンロードしている場合は容量にご注意ください。\n",
    "# 具体的には以下のフォルダを適宜消す、あるいは/workspace/assets以下に移動するなどしてください。\n",
    "os.environ['HF_HOME'] = '/workspace/hf'\n",
    "\n",
    "!pip install -q \"optimum>=1.12.0\" openpyxl==3.1.2\n",
    "!pip install -q auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiaH3qOo2H4x"
   },
   "source": [
    "## ファインチューニングモデルにおける学習データの利用および事後学習における学習データの利用可否をチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isGHmmWN2H4y"
   },
   "outputs": [],
   "source": [
    "#他者公開のモデルにおける違反データ利用有無のチェック\n",
    "#1行でも同じデータがあった場合、違反データとしてFalseを設定し、対象データセット名を表示\n",
    "from datasets import load_dataset\n",
    "\n",
    "def confirm_data_usage(dataset):\n",
    "\n",
    "    dataset = list(set(dataset))\n",
    "    flag = True\n",
    "\n",
    "    #jglue\n",
    "    check_data=\"shunk031/JGLUE\"\n",
    "    name=\"MARC-ja\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check[\"sentence\"]))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    name=\"JCoLA\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['sentence']+check['original']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    jcola = load_dataset(check_data, name=name, split=split)['original']\n",
    "\n",
    "    split=\"validation_out_of_domain\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['sentence']+check['original']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    split=\"validation_out_of_domain_annotated\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['sentence']+check['original']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    name=\"JSTS\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['sentence1']+check['sentence2']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    name=\"JNLI\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['sentence1']+check['sentence2']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    name=\"JSQuAD\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['context']+check['question']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    name=\"JCommonsenseQA\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['question']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    #xlsum\n",
    "    check_data=\"csebuetnlp/xlsum\"\n",
    "    name=\"japanese\"\n",
    "    split=\"validation\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['text']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    split=\"test\"\n",
    "    check = load_dataset(check_data, name=name, split=split)\n",
    "    check = list(set(check['text']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    #elyza\n",
    "    check_data=\"elyza/ELYZA-tasks-100\"\n",
    "    name=\"\"\n",
    "    split=\"test\"\n",
    "    check = load_dataset(check_data, split=split)\n",
    "    check = list(set(check['input']+check['output']))\n",
    "    if len(dataset+check) != len(set(dataset+check)):\n",
    "        print(\"xxx NG xxx\")\n",
    "        print(\"     check_data:\",check_data)\n",
    "        print(\"     name:\",name)\n",
    "        print(\"     split:\",split)\n",
    "        flag = False\n",
    "\n",
    "    if flag: print(\"ooo OK ooo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUhZynqKQQgj"
   },
   "outputs": [],
   "source": [
    "#重複した行を表示\n",
    "def get_duplicate_row(train, test):\n",
    "    dup = set(train) & set(test)\n",
    "    print(\"--- train_row : text ---\")\n",
    "    print([str(i) + \":\" +  text for i,text in enumerate(train) if text in dup])\n",
    "    print(\"--- test_row : text ---\")\n",
    "    print([str(i) + \":\" +  text for i,text in enumerate(test) if text in dup])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLeGGDq-2H40"
   },
   "source": [
    "### [lightblue/openorca_stx](https://huggingface.co/lightblue/openorca_stx)におけるFT利用データを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "68ad09afd8394da98b6e08037c312cf0",
      "76c97fa8a9724348a5f0c8c18d389539",
      "4f775715f4d64f3abffd8f7739ea3008",
      "f0f9323fd5394d7b9526e988ef1f259d",
      "bf5bce893b804002bf56d4bcccfdc75e",
      "5500a6c1957d4eadb36451ad5ce06469",
      "d0ddbfacdc42433c85d6c4e80ae3c6c9",
      "6f02a1f498364a089b26fabc6302ba6b",
      "6e8fa9245e0c4936a6b0d9f5c8c6e5a2",
      "96a9f661ceb34b67bc9e894046f72fed",
      "86f127f40e2f4949a05b9fd3cd8f5498",
      "d29e95884da74f9d85de8ec7042be28a",
      "863c27b5ddbf4d29ad999c68035f104c",
      "35ad48c3d9bf4bd595b64196d464b9ba",
      "14d74ae1504144249b03e49446264c7b",
      "019718bc6c964bb3aff1f0affba7c9ff",
      "a89cb7f4cf8d40cdbed122e2c1b11d59",
      "4765ed24ebbf4a7fbd1e66b74e2da194",
      "bd62eca66daf433084f6ac77a4109566",
      "fee2a3d3d5ae49dcbed5e54f04930eb2",
      "7e1eaa8a9a1c4731b23acaec43b04f3e",
      "3278cdaf5f8443c1b7bcd29484bf29b1",
      "3f0a0eacc5f64627b6a69e44df0be6db",
      "8ed7f6d680764c00ac266804ae54158d",
      "e4a5a0f7cfaa4fb18a2e8c941dc7c121",
      "b54db4d53cdf48419c7b3efac900c0b3",
      "37224db45f694610a1cca8d20981fff3",
      "3984b6a05bc84a719aee4c1c3f02a664",
      "e16d2f277f3c429d8de6118836d90aa7",
      "f48c42a38d9343ba982f0f408c955e3b",
      "b88c223f39374dda82d5253af7c9b70c",
      "f9ddde5c1d0a41ebb10914f438a54c73",
      "7a9f65a65cd14d7c899912e1389dfc89",
      "78e273c60fa04a608042646ccf9e4d6a",
      "26b38f44160344deb92c073876d1c9ab",
      "3290ec517b13451ca70965fa4c6bbb06",
      "07eba8be09dd4530b8e9d6d32a653c4f",
      "1e266af486f84e8785599d7481b457c9",
      "67d1556c2c7845ca949ebebe2c2d2992",
      "a0b5640454d4493e97bdf768cf404ec1",
      "6140e5e6cfcc47bb9c84013f31aa8ab9",
      "f7150d83368e437c88435edf3edad7fe",
      "0e1e704d301b45a7ab4cf86c39564db7",
      "8cb72668c6604dcca336fa3e933a8dfd",
      "63df95ef66a14d8b84b76fb60d5ffe4c",
      "54f994e7eed6447f805c1e838219fb31",
      "afb3999f3f544a6b891c3e950856ce30",
      "668e2fd743564110a316ed4bdf80a0e9",
      "a0d21f1f48fc4e7eba6986c54df167a1",
      "60595063bc8a463192bbf5cf216f6114",
      "583db48def6f45ffa7130369ac5c4d94",
      "19dd0a6d48b347459a8fd73a6d6e0f7c",
      "d5471e25fde545d986c374d5d2b01df9",
      "a6fb12b154af46359e4ca22deb3c717a",
      "2a0cd9541ba2476e88ef68a955912942",
      "f0536fc5e64f47cf9a503e6cbde697f2",
      "1c0afe073b8540a8948d0904c3ccdb3f",
      "9b3b834cad27416b9945e731fdebd079",
      "e5e7590f80fa49b7b51c50674c4bed40",
      "55ff0259c9d4481480223f4c8657fc6c",
      "1347d452fbcd4425a7e1f1fd8811c012",
      "90d3d767f1ce4a90a334417355540da8"
     ]
    },
    "id": "bRjqBCyP2H41",
    "outputId": "29921592-8b4e-4231-a7b8-20e6eaeeb2ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ad09afd8394da98b6e08037c312cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c97fa8a9724348a5f0c8c18d389539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/8.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f775715f4d64f3abffd8f7739ea3008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f9323fd5394d7b9526e988ef1f259d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.63M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf5bce893b804002bf56d4bcccfdc75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "# snow_t15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5500a6c1957d4eadb36451ad5ce06469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/28.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ddbfacdc42433c85d6c4e80ae3c6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/38.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f02a1f498364a089b26fabc6302ba6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8fa9245e0c4936a6b0d9f5c8c6e5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a9f661ceb34b67bc9e894046f72fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.09k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f127f40e2f4949a05b9fd3cd8f5498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.42k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29e95884da74f9d85de8ec7042be28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"None of [Index(['review_body', 'star_rating', 'review_id'], dtype='object')] are in the [columns]\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863c27b5ddbf4d29ad999c68035f104c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ad48c3d9bf4bd595b64196d464b9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/65.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d74ae1504144249b03e49446264c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019718bc6c964bb3aff1f0affba7c9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89cb7f4cf8d40cdbed122e2c1b11d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4765ed24ebbf4a7fbd1e66b74e2da194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd62eca66daf433084f6ac77a4109566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee2a3d3d5ae49dcbed5e54f04930eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/201k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1eaa8a9a1c4731b23acaec43b04f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3278cdaf5f8443c1b7bcd29484bf29b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/19.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0a0eacc5f64627b6a69e44df0be6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/52.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed7f6d680764c00ac266804ae54158d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a5a0f7cfaa4fb18a2e8c941dc7c121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54db4d53cdf48419c7b3efac900c0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37224db45f694610a1cca8d20981fff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_out_of_domain split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3984b6a05bc84a719aee4c1c3f02a664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_out_of_domain_annotated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxx NG xxx\n",
      "     check_data: shunk031/JGLUE\n",
      "     name: JCoLA\n",
      "     split: validation\n",
      "xxx NG xxx\n",
      "     check_data: shunk031/JGLUE\n",
      "     name: JCoLA\n",
      "     split: validation_out_of_domain\n",
      "xxx NG xxx\n",
      "     check_data: shunk031/JGLUE\n",
      "     name: JCoLA\n",
      "     split: validation_out_of_domain_annotated\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16d2f277f3c429d8de6118836d90aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48c42a38d9343ba982f0f408c955e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/653k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88c223f39374dda82d5253af7c9b70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ddde5c1d0a41ebb10914f438a54c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9f65a65cd14d7c899912e1389dfc89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e273c60fa04a608042646ccf9e4d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b38f44160344deb92c073876d1c9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3290ec517b13451ca70965fa4c6bbb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/700k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eba8be09dd4530b8e9d6d32a653c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/93.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e266af486f84e8785599d7481b457c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d1556c2c7845ca949ebebe2c2d2992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b5640454d4493e97bdf768cf404ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6140e5e6cfcc47bb9c84013f31aa8ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7150d83368e437c88435edf3edad7fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1e704d301b45a7ab4cf86c39564db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/353k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb72668c6604dcca336fa3e933a8dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63df95ef66a14d8b84b76fb60d5ffe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f994e7eed6447f805c1e838219fb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb3999f3f544a6b891c3e950856ce30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668e2fd743564110a316ed4bdf80a0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/488k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d21f1f48fc4e7eba6986c54df167a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/62.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60595063bc8a463192bbf5cf216f6114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "583db48def6f45ffa7130369ac5c4d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dd0a6d48b347459a8fd73a6d6e0f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5471e25fde545d986c374d5d2b01df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fb12b154af46359e4ca22deb3c717a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0cd9541ba2476e88ef68a955912942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0536fc5e64f47cf9a503e6cbde697f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0afe073b8540a8948d0904c3ccdb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3b834cad27416b9945e731fdebd079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e7590f80fa49b7b51c50674c4bed40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/8.37k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ff0259c9d4481480223f4c8657fc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1347d452fbcd4425a7e1f1fd8811c012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/115k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d3d767f1ce4a90a334417355540da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SNOW T15：原文（日本語）をチェック\n",
    "train_data = load_dataset(\"snow_simplified_japanese_corpus\", name=\"snow_t15\", split=\"train\")\n",
    "print(\"-\"*20)\n",
    "print(\"# snow_t15\")\n",
    "check = confirm_data_usage(train_data[\"original_ja\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "00335a93b0334f988369de4d800603fd",
      "31b5bbabfca047ebb7c209fbd55e10a3"
     ]
    },
    "id": "r9ZwMMALfg86",
    "outputId": "2156c974-58cc-4c76-c015-79197b47e91f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00335a93b0334f988369de4d800603fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.64M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b5bbabfca047ebb7c209fbd55e10a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/34300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "# snow_t23\n",
      "ooo OK ooo\n"
     ]
    }
   ],
   "source": [
    "#SNOW T23：原文（日本語）をチェック\n",
    "train_data = load_dataset(\"snow_simplified_japanese_corpus\", name=\"snow_t23\", split=\"train\")\n",
    "print(\"-\"*20)\n",
    "print(\"# snow_t23\")\n",
    "check = confirm_data_usage(train_data[\"original_ja\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "6032f0f368fa4aebb2ba18c3aa0245e5",
      "774fb090c6434681979cd61eada3bedf",
      "fd217e47eb344581b088e585984c6fa0",
      "3a84baf87a804ed1a8f48fcb92a6c73c",
      "8de0a9252d294179a88f3d4bb31638fe",
      "0d06b0607f5d460a8b070cfd0a80cd65"
     ]
    },
    "id": "1dkTrqrDfg87",
    "outputId": "c3249059-3432-4138-e92f-4c86547cde86"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6032f0f368fa4aebb2ba18c3aa0245e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774fb090c6434681979cd61eada3bedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/3.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd217e47eb344581b088e585984c6fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a84baf87a804ed1a8f48fcb92a6c73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de0a9252d294179a88f3d4bb31638fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d06b0607f5d460a8b070cfd0a80cd65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/485k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooo OK ooo\n"
     ]
    }
   ],
   "source": [
    "#TyDiQA (Ja)：質問文をチェック\n",
    "train_data = load_dataset(\"khalidalt/tydiqa-goldp\", name=\"japanese\", split=\"train\")\n",
    "print(\"-\"*20)\n",
    "print(\"# tydiqa\")\n",
    "check = confirm_data_usage(train_data[\"question_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iu1DXQKofg88",
    "outputId": "64932e44-6201-4d68-8f8f-71d8c07189d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "# xlsum\n",
      "ooo OK ooo\n"
     ]
    }
   ],
   "source": [
    "#XLSUM (Ja)：テキスト（原文）をチェック\n",
    "train_data = load_dataset(\"csebuetnlp/xlsum\", \"japanese\", split=\"train\")\n",
    "print(\"-\"*20)\n",
    "print(\"# xlsum\")\n",
    "check = confirm_data_usage(train_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5oA5W4RvAfcO",
    "outputId": "23536662-dad2-4f4f-de8a-a98b80842663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "# validation\n",
      "--- train_row : text ---\n",
      "['49586:今日は天気が良い。']\n",
      "--- test_row : text ---\n",
      "['330:今日は天気が良い。']\n",
      "####################\n",
      "# validation_out_of_domain\n",
      "--- train_row : text ---\n",
      "['49810:ドアが開いた。']\n",
      "--- test_row : text ---\n",
      "['422:ドアが開いた。']\n",
      "####################\n",
      "# validation_out_of_domain_annotated\n",
      "--- train_row : text ---\n",
      "['49810:ドアが開いた。']\n",
      "--- test_row : text ---\n",
      "['422:ドアが開いた。']\n"
     ]
    }
   ],
   "source": [
    "#snow_t15の重複チェック\n",
    "train_data = load_dataset(\"snow_simplified_japanese_corpus\", name=\"snow_t15\", split=\"train\")\n",
    "test_data = load_dataset(\"shunk031/JGLUE\", name=\"JCoLA\", split=\"validation\")\n",
    "print(\"#\"*20)\n",
    "print(\"# validation\")\n",
    "get_duplicate_row(train_data[\"original_ja\"], test_data[\"original\"])\n",
    "\n",
    "test_data = load_dataset(\"shunk031/JGLUE\", name=\"JCoLA\", split=\"validation_out_of_domain\")\n",
    "print(\"#\"*20)\n",
    "print(\"# validation_out_of_domain\")\n",
    "get_duplicate_row(train_data[\"original_ja\"], test_data[\"original\"])\n",
    "\n",
    "test_data = load_dataset(\"shunk031/JGLUE\", name=\"JCoLA\", split=\"validation_out_of_domain_annotated\")\n",
    "print(\"#\"*20)\n",
    "print(\"# validation_out_of_domain_annotated\")\n",
    "get_duplicate_row(train_data[\"original_ja\"], test_data[\"original\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs-kbYhzU923"
   },
   "source": [
    "#### ※重複があったが、内容がよくある文章であり、タスクも違う文章のため、対象モデルは利用可能と判断\n",
    "- SNOW：要約\n",
    "- JCoLA：文書分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3lx2pZ7pJnB"
   },
   "source": [
    "### [TheBloke/StableBeluga2-70B-GPTQ](https://huggingface.co/lightblue/openorca_stx)におけるFT利用データを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7B5CPZL9pJnB"
   },
   "source": [
    "#### ※[Orca-styleのデータセット](https://huggingface.co/stabilityai/StableBeluga2#training-dataset)は固有データかつ英語データのため、\n",
    "#### 　チェックは割愛し、対象モデルは利用可能と判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ve8ZAYbIZUCJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "del train_data\n",
    "del test_data\n",
    "del check\n",
    "del confirm_data_usage\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01deLksUfg8_",
    "outputId": "52092b54-2d0c-4a01-e5f3-2c159299614f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Oct  9 04:43:53 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   60C    P8              14W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P8              12W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA L4                      Off | 00000000:00:05.0 Off |                    0 |\n",
      "| N/A   47C    P8              12W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA L4                      Off | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   54C    P8              13W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuVrVlkg2H43"
   },
   "source": [
    "## モデル（StableBeluga2-70B-GPTQ）ダウンロード・読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "890064c1745a414280c1789734e7165d",
      "644d97c40c32495fa02e74dfe46b6097",
      "f7f06995bf67492f90abb2e160d6d31c",
      "0d81b237463f4e679e3fc3445fa5634f",
      "b3c742ac9144413f8c68c3ec87a2b5b9",
      "aeb68b7f887641e2a6f0150c8440d02b",
      "0f5ad9c587c941d1adbd581351aeed6a",
      "617a6f2d0264447286421921b472e108",
      "ed50f7e90282458eb42886b8919fe05e",
      "a92f4e7b959644b8af4d24bb301fcd8b",
      "67465c11438b489d8b621ed3de871bab",
      "e4ecf6e2d7e542d2b0c551a2a71b8bb1",
      "2caaa05ac6e44373b90c495fa86601bb",
      "4571c0b825124b6d94605ec5b3530e26",
      "057578b43b5742458664aeeb3c6a805c"
     ]
    },
    "id": "cUwONFfGj7Nk",
    "outputId": "524fb723-ce4e-4c3c-afc7-f8356abcf51c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38308a0f2202451b8b2b29f1130492ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6ad2df0b084a599882e9178df9e05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabe8335ec9a4420b3ee6e19bc5f341a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562e599f58da47718ab1ccb18f6b30f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f674bbb08fe48deb284b9fcf50b578c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/888 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-13 00:37:57,268] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f1a61cb5594a23a5ba45bdbc4355bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/35.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a5d90fc7694dfa8166c712093c19da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "ロード時間\n",
      "155.77s\n",
      "2.60m\n",
      "0.04h\n"
     ]
    }
   ],
   "source": [
    "# モデルのダウンロードと読み込み\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed, logging\n",
    "import torch\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_name = \"TheBloke/StableBeluga2-70B-GPTQ\"\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "bnb_4bit_compute_dtype=\"bfloat16\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "use_fast=True\n",
    "revision=\"main\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto',\n",
    "    #quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    use_safetensors=True,\n",
    "    revision=revision,\n",
    "    )\n",
    "\n",
    "sec_load1 = time.time() - start_time\n",
    "print(\"#\"*30)\n",
    "print('ロード時間')\n",
    "print(f\"{sec_load1:.2f}s\")\n",
    "print(f\"{sec_load1/60:.2f}m\")\n",
    "print(f\"{sec_load1/60/60:.2f}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "b4c1GBQoj7Nl",
    "outputId": "638e3ca3-4586-4aa8-8eea-63b7a2863934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 13 00:40:19 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   59C    P0              30W /  72W |  16403MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   52C    P0              29W /  72W |  17963MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BrfbpxlMLy9X",
    "outputId": "09b66e94-8ee2-4bde-a5bf-28b1293c9ab2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 0,\n",
       " 'model.layers.4': 0,\n",
       " 'model.layers.5': 0,\n",
       " 'model.layers.6': 0,\n",
       " 'model.layers.7': 0,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 0,\n",
       " 'model.layers.10': 0,\n",
       " 'model.layers.11': 0,\n",
       " 'model.layers.12': 0,\n",
       " 'model.layers.13': 0,\n",
       " 'model.layers.14': 0,\n",
       " 'model.layers.15': 0,\n",
       " 'model.layers.16': 0,\n",
       " 'model.layers.17': 0,\n",
       " 'model.layers.18': 0,\n",
       " 'model.layers.19': 0,\n",
       " 'model.layers.20': 0,\n",
       " 'model.layers.21': 0,\n",
       " 'model.layers.22': 0,\n",
       " 'model.layers.23': 0,\n",
       " 'model.layers.24': 0,\n",
       " 'model.layers.25': 0,\n",
       " 'model.layers.26': 0,\n",
       " 'model.layers.27': 0,\n",
       " 'model.layers.28': 0,\n",
       " 'model.layers.29': 0,\n",
       " 'model.layers.30': 0,\n",
       " 'model.layers.31': 0,\n",
       " 'model.layers.32': 0,\n",
       " 'model.layers.33': 0,\n",
       " 'model.layers.34': 0,\n",
       " 'model.layers.35': 0,\n",
       " 'model.layers.36': 0,\n",
       " 'model.layers.37': 0,\n",
       " 'model.layers.38': 1,\n",
       " 'model.layers.39': 1,\n",
       " 'model.layers.40': 1,\n",
       " 'model.layers.41': 1,\n",
       " 'model.layers.42': 1,\n",
       " 'model.layers.43': 1,\n",
       " 'model.layers.44': 1,\n",
       " 'model.layers.45': 1,\n",
       " 'model.layers.46': 1,\n",
       " 'model.layers.47': 1,\n",
       " 'model.layers.48': 1,\n",
       " 'model.layers.49': 1,\n",
       " 'model.layers.50': 1,\n",
       " 'model.layers.51': 1,\n",
       " 'model.layers.52': 1,\n",
       " 'model.layers.53': 1,\n",
       " 'model.layers.54': 1,\n",
       " 'model.layers.55': 1,\n",
       " 'model.layers.56': 1,\n",
       " 'model.layers.57': 1,\n",
       " 'model.layers.58': 1,\n",
       " 'model.layers.59': 1,\n",
       " 'model.layers.60': 1,\n",
       " 'model.layers.61': 1,\n",
       " 'model.layers.62': 1,\n",
       " 'model.layers.63': 1,\n",
       " 'model.layers.64': 1,\n",
       " 'model.layers.65': 1,\n",
       " 'model.layers.66': 1,\n",
       " 'model.layers.67': 1,\n",
       " 'model.layers.68': 1,\n",
       " 'model.layers.69': 1,\n",
       " 'model.layers.70': 1,\n",
       " 'model.layers.71': 1,\n",
       " 'model.layers.72': 1,\n",
       " 'model.layers.73': 1,\n",
       " 'model.layers.74': 1,\n",
       " 'model.layers.75': 1,\n",
       " 'model.layers.76': 1,\n",
       " 'model.layers.77': 1,\n",
       " 'model.layers.78': 1,\n",
       " 'model.layers.79': 1,\n",
       " 'model.norm': 1,\n",
       " 'lm_head': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kbBVYo6h2H47",
    "outputId": "0b8174b7-feb3-4a0f-fa09-927959257aae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 8192, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-79): 80 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "          (k_proj): QuantLinear()\n",
       "          (o_proj): QuantLinear()\n",
       "          (q_proj): QuantLinear()\n",
       "          (v_proj): QuantLinear()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (act_fn): SiLUActivation()\n",
       "          (down_proj): QuantLinear()\n",
       "          (gate_proj): QuantLinear()\n",
       "          (up_proj): QuantLinear()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8192, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsO6l_KUpJnI"
   },
   "source": [
    "## 推論用関数（生成タスク）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cYlvvvJfpJnI"
   },
   "outputs": [],
   "source": [
    "def generation(model, tokenizer, test_data, params):\n",
    "# モデル・GPUメモリによって扱える最大トークン数が異なるので注意\n",
    "# タスクによっては入力文が長いものがあります、どのように対処するかは各自で工夫いただければと思います。\n",
    "# token長の2乗に計算量とメモリが必要になるので、提出用の推論処理のGPU資源確保の際にはご注意ください。\n",
    "# max_length = 2048\n",
    "#max_length = 256\n",
    "\n",
    "    count = 0\n",
    "    for data in test_data:\n",
    "        count += 1\n",
    "\n",
    "        if data['task_type'] == 'generation':\n",
    "            print(\"id :\",count-1)\n",
    "            max_length = 1024+128\n",
    "\n",
    "            text = f\"\"\"\\nまず以下の文章の内容を読み取り、正しく理解してください。そして適切な日本語で簡潔に回答してください。\n",
    "            \\n\\n{data['text']}\n",
    "            \\n回答:\"\"\"\n",
    "\n",
    "            token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "            token_ids = token_ids[:, -(max_length):]\n",
    "            token_ids_size = len(token_ids[0])\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    inputs=token_ids.to(model.device),\n",
    "                    max_new_tokens=256,\n",
    "                    min_new_tokens=1,\n",
    "                    do_sample=False,\n",
    "                    top_k=50,\n",
    "                    top_p=0.98,\n",
    "                    num_beams=params[\"num_beams\"],\n",
    "                    early_stopping=True,\n",
    "                    no_repeat_ngram_size=params[\"no_repeat_ngram_size\"],\n",
    "                    repetition_penalty=params[\"repetition_penalty\"],\n",
    "                    max_time=params[\"max_time\"],\n",
    "                )\n",
    "\n",
    "            print(f\"{time.time() - start_time:.2f}s\")\n",
    "            del token_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            output = tokenizer.decode(output_ids.tolist()[0][token_ids_size:], skip_special_tokens=True)\n",
    "            del output_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            print(output)\n",
    "            print(\"#\"*20)\n",
    "        else:\n",
    "            output=\"\"\n",
    "\n",
    "        data['answer'] = output\n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHqkkYW-pJnJ"
   },
   "source": [
    "## 生成タスクの推論（Type3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LO2RI-OCpJnK",
    "outputId": "54e95491-dad7-40fd-eab2-b3ad333702b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 70\n",
      "330.25s\n",
      "「凄まじい勢いで吹き荒れる風が」\n",
      "####################\n",
      "id : 71\n",
      "688.47s\n",
      "\n",
      "1. 目標を明確に定める\n",
      "2. 仕事に興味を持つための新しいスキルを学ぶ\n",
      "3. 同僚や上司とのコミュニケーションを強化する\n",
      "4. 自分の仕事がどう社会に貢献しているかを理解する\n",
      "5. バランスのとれた生活を送る\n",
      "####################\n",
      "id : 72\n",
      "281.70s\n",
      "「あなたが本当にやりたいことを見つけたら、私はどんなに応援してあげるつもりだわ。」\n",
      "####################\n",
      "id : 73\n",
      "516.26s\n",
      "\n",
      "\n",
      "古代エジプトを学ぶには、その歴史、文化、芸術、建築、哲学、政治などの側面を理解する必要があります。また、その時代の重要な人物や出来事についても調べるべきです。\n",
      "####################\n",
      "id : 74\n",
      "109.45s\n",
      "\n",
      "1. 南\n",
      "2. 東\n",
      "####################\n",
      "id : 75\n",
      "344.51s\n",
      "私たちは全力を尽くして安全性を確保し、リソースを活用して環境負荷の軽揚を図ります。\n",
      "####################\n",
      "id : 76\n",
      "903.78s\n",
      "LLM (Language Learning with Machines) を使って英語を学ぶ初心者にとっての利点は、自分のペースで学習できる自律学習が可能であること、また自動評価システムによって正確さを向上させることが挙げられます。一方で、欠点としては、人間の先生が提供するような個人的なフィードバックが得られないことや、コミュニケーション能力の向上\n",
      "####################\n",
      "id : 77\n",
      "441.16s\n",
      "祖母に会えないことを残念に思っていること、近況を説明すること、祖母の健康状態について心配することなどを書くことができます。\n",
      "####################\n",
      "id : 78\n",
      "904.14s\n",
      "\n",
      "\n",
      "1. プレイヤが互いに協力して謎を解き、最終的にボスを倒すアドベンチャーゲーム。\n",
      "2. 各プレイヤは異なる特殊能力を持ち、それらを組み合わせて課題を解決するパズルゲーム。 \n",
      "3. オンラインでプレイできるコオペレーションマルチプレイヤRPG。\n",
      "####################\n",
      "id : 79\n",
      "290.63s\n",
      "私たちは、子供たちが喜んで聞くことができるような、楽しいお話を作りました。\n",
      "####################\n",
      "id : 80\n",
      "860.84s\n",
      "平安時代は、日本の文化や政治に大きな変化をもたらした時代です。この時代には、天皇の権力が強まり、貴族制度が発展しました。また、仏教の影響が強くなり、絵画や彫刻、建築などの芸術分野でも大きな進歩がありました。平安文化と呼ばれる独自の文化が花開いた時代でもあります。\n",
      "####################\n",
      "id : 81\n",
      "903.75s\n",
      "田舎の高校生活では、自然に囲まれた環境での生活や、地域コミュニティとの繋がりが強いメリットがあります。しかし、文化的な活動やインフラストラクチャーが都会よりも限られるなどのデメリッ\n",
      "トもあり得ます。一方で、都会の高校では、多様な文化的な経験や、進学や就職に関する機会が豊富であるメリット\n",
      "####################\n",
      "id : 82\n",
      "200.77s\n",
      "重いもの：トラック、クジラ\n",
      "軽くもの：紙、シャーブペンシル\n",
      "####################\n",
      "id : 83\n",
      "242.31s\n",
      "サッカーは、2チームがボールを使ってゴールに入れるポイントを競い合う球技です。\n",
      "####################\n",
      "id : 84\n",
      "769.60s\n",
      "\n",
      "1. スポーツ：体を動かして健康を維持し、チームワークや競争心も養うことができます。\n",
      "2. 絵画：創造性や表現力を高め、自己表現の場を提供します。\n",
      "3. 音楽：聴覚やリズム感を養い、集中力や協調性も向上させることが可能です。\n",
      "####################\n",
      "id : 85\n",
      "667.03s\n",
      "この式は、文字列「output」から'1'、'2'、'3'、'4'、'5'のいずれかが含まれる文字を抽出し、それを整数に変換した後、その整数のリストを生成します。そして、そのリストからランダムに要素を選択し、その値を返します。\n",
      "####################\n",
      "id : 86\n",
      "329.41s\n",
      "この場面での「大仲良い」とは、顧客がもう腹一杯なのでデザートを注文しないつもりだという意味です。\n",
      "####################\n",
      "id : 87\n",
      "903.96s\n",
      "ゴルフを続けることで技術が向上し、スコアを低くすることができるようになります。レッスンを受けたり、他のプレイヤーとの練習試合を行ったりすることで、自分の課題を見つけて克服できるかもしれません。\n",
      "####################\n",
      "id : 88\n",
      "903.62s\n",
      "LLMとは、自然言語処理や自然言語理解の分野で使用される、複数の言語を扱うことができる高度なアルゴリズムやモデルのことです。これらのモデルは、大����量のデータを学習し、言語の複雑な構造や意味を理解することが可能になります。 LLMは、テキスト生成、意味解析、翻訳などのタスクに応用され、\n",
      "####################\n",
      "id : 89\n",
      "194.37s\n",
      "サッカー用品、サッカーウェア、フットサルシューズなど。\n",
      "####################\n",
      "##############################\n",
      "推論時間\n",
      "10787.45s\n",
      "179.79m\n",
      "3.00h\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"num_beams\":3,\n",
    "    \"no_repeat_ngram_size\":5,\n",
    "    \"repetition_penalty\":1.1,\n",
    "    \"max_time\":60*15,\n",
    "}\n",
    "\n",
    "# 推論を行うデータの確認、テストデータはコンペ期間中に追加される可能性があります、追加した際はslackにてアナウンスを行うのでご確認ください。\n",
    "test_data = json.load(open('new_test.json', 'r'))\n",
    "\n",
    "generation_data = generation(model, tokenizer, test_data, params)\n",
    "\n",
    "sec_gene = time.time() - start_time\n",
    "print(\"#\"*30)\n",
    "print('推論時間')\n",
    "print(f\"{sec_gene:.2f}s\")\n",
    "print(f\"{sec_gene/60:.2f}m\")\n",
    "print(f\"{sec_gene/60/60:.2f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vJwWfrHu9Kb"
   },
   "source": [
    "## 結果を保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XuU3fes6pJnK",
    "outputId": "486955a7-15e4-4c58-889d-a403a2dd7250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "--------------------\n",
      "{'id': 0, 'task_type': 'multiple_choice', 'text': '物の外側をなす面を何と言うか？', 'choices': [{'choice_id': 1, 'text': '平面'}, {'choice_id': 2, 'text': '対面'}, {'choice_id': 3, 'text': '体'}, {'choice_id': 4, 'text': '表面'}, {'choice_id': 5, 'text': '顔面'}], 'answer': ''}\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_submission_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheck_submission_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_submission_data[\u001b[38;5;241m125\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 推論結果の保存、id, task_type, text, answerのkeyがあることを確認してください(正しく入っていない場合、スコア付けが行われません)\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(generation_data, f, indent=4)\n",
    "\n",
    "check_submission_data = json.load(open('submission.json', 'r'))\n",
    "print(len(check_submission_data))\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[0])\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[101])\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4zxT1_4RXRlz",
    "outputId": "e90e5076-8338-411d-9d84-e6f41c8e5d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Oct 13 03:45:25 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   77C    P0              35W /  72W |  16503MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0              34W /  72W |  18029MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biXBmClQXRlz"
   },
   "source": [
    "## 推論用関数（選択タスク）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZgHSrU95XRlz"
   },
   "outputs": [],
   "source": [
    "def choice(model, tokenizer, test_data):\n",
    "\n",
    "    count = 0\n",
    "    for data in test_data:\n",
    "        count += 1\n",
    "\n",
    "        if data['task_type'] == 'multiple_choice':\n",
    "            print(\"id :\",count-1)\n",
    "            max_length = 1\n",
    "            text =  f\"あなたはクイズや常識問題が得意です。以下の例題のように適切な回答を出力してください。\\n##########\\n[問題]:日本の首都は？ \\n[選択肢]:[1. ワシントン, 2. 東京, 3. ニューヨーク, 4. パリ, 5. 京都] \\n[答えの選択肢番号]:2 \\n（解説:この場合、日本の都道府県であり、かつ首都である選択肢は[2. 東京]だけなので、2が正解になります。）\\n##########　\\n[問題]:{data['text']}\\n[選択肢]:[{data['choices'][0]['choice_id']}. {data['choices'][0]['text']}, {data['choices'][1]['choice_id']}. {data['choices'][1]['text']}, {data['choices'][2]['choice_id']}. {data['choices'][2]['text']}, {data['choices'][3]['choice_id']}. {data['choices'][3]['text']}, {data['choices'][4]['choice_id']}. {data['choices'][4]['text']}]\\n[答えの選択肢番号]:\"\n",
    "            token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "            token_ids_size = len(token_ids[0])\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    inputs=token_ids.to(model.device),\n",
    "                    max_new_tokens=max_length,\n",
    "                    do_sample=False,\n",
    "                    min_new_tokens=max_length,\n",
    "                )\n",
    "\n",
    "            print(f\"{time.time() - start_time:.2f}s\")\n",
    "            del token_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            output = tokenizer.decode(output_ids.tolist()[0][token_ids_size:], skip_special_tokens=True)\n",
    "            del output_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            print(output)\n",
    "            print(\"#\"*20)\n",
    "        else:\n",
    "            output=\"\"\n",
    "\n",
    "        # 出力が1~5の数字になるようにする\n",
    "        data['answer'] = next((int(char) for char in output if char in '12345'), random.randint(1, 5))\n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dczCNukzpJnL"
   },
   "source": [
    "## 選択タスクの推論（Type1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "VhkFexUxpJnL",
    "outputId": "121e5f4b-6a17-4aea-b520-a7b41c299401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 0\n",
      "7.07s\n",
      "4\n",
      "####################\n",
      "id : 1\n",
      "6.04s\n",
      "1\n",
      "####################\n",
      "id : 2\n",
      "6.06s\n",
      "1\n",
      "####################\n",
      "id : 3\n",
      "6.03s\n",
      "3\n",
      "####################\n",
      "id : 4\n",
      "6.01s\n",
      "4\n",
      "####################\n",
      "id : 5\n",
      "5.97s\n",
      "5\n",
      "####################\n",
      "id : 6\n",
      "5.94s\n",
      "5\n",
      "####################\n",
      "id : 7\n",
      "5.95s\n",
      "5\n",
      "####################\n",
      "id : 8\n",
      "5.91s\n",
      "4\n",
      "####################\n",
      "id : 9\n",
      "5.92s\n",
      "1\n",
      "####################\n",
      "id : 10\n",
      "5.95s\n",
      "1\n",
      "####################\n",
      "id : 11\n",
      "5.96s\n",
      "3\n",
      "####################\n",
      "id : 12\n",
      "5.95s\n",
      "5\n",
      "####################\n",
      "id : 13\n",
      "6.00s\n",
      "5\n",
      "####################\n",
      "id : 14\n",
      "6.01s\n",
      "1\n",
      "####################\n",
      "id : 15\n",
      "6.03s\n",
      "3\n",
      "####################\n",
      "id : 16\n",
      "6.01s\n",
      "2\n",
      "####################\n",
      "id : 17\n",
      "6.02s\n",
      "4\n",
      "####################\n",
      "id : 18\n",
      "6.00s\n",
      "4\n",
      "####################\n",
      "id : 19\n",
      "5.97s\n",
      "4\n",
      "####################\n",
      "id : 20\n",
      "5.97s\n",
      "4\n",
      "####################\n",
      "id : 21\n",
      "6.01s\n",
      "5\n",
      "####################\n",
      "id : 22\n",
      "5.93s\n",
      "2\n",
      "####################\n",
      "id : 23\n",
      "5.97s\n",
      "1\n",
      "####################\n",
      "id : 24\n",
      "6.00s\n",
      "2\n",
      "####################\n",
      "id : 25\n",
      "5.93s\n",
      "5\n",
      "####################\n",
      "id : 26\n",
      "5.92s\n",
      "1\n",
      "####################\n",
      "id : 27\n",
      "5.99s\n",
      "3\n",
      "####################\n",
      "id : 28\n",
      "5.95s\n",
      "4\n",
      "####################\n",
      "id : 29\n",
      "5.94s\n",
      "2\n",
      "####################\n",
      "id : 30\n",
      "6.01s\n",
      "3\n",
      "####################\n",
      "id : 31\n",
      "6.01s\n",
      "3\n",
      "####################\n",
      "id : 32\n",
      "6.04s\n",
      "1\n",
      "####################\n",
      "id : 33\n",
      "6.03s\n",
      "3\n",
      "####################\n",
      "id : 34\n",
      "6.03s\n",
      "1\n",
      "####################\n",
      "id : 35\n",
      "6.01s\n",
      "2\n",
      "####################\n",
      "id : 36\n",
      "6.03s\n",
      "2\n",
      "####################\n",
      "id : 37\n",
      "6.01s\n",
      "1\n",
      "####################\n",
      "id : 38\n",
      "6.02s\n",
      "1\n",
      "####################\n",
      "id : 39\n",
      "6.00s\n",
      "4\n",
      "####################\n",
      "id : 40\n",
      "5.97s\n",
      "1\n",
      "####################\n",
      "id : 41\n",
      "5.98s\n",
      "2\n",
      "####################\n",
      "id : 42\n",
      "6.01s\n",
      "4\n",
      "####################\n",
      "id : 43\n",
      "6.03s\n",
      "4\n",
      "####################\n",
      "id : 44\n",
      "5.96s\n",
      "3\n",
      "####################\n",
      "id : 45\n",
      "5.98s\n",
      "2\n",
      "####################\n",
      "id : 46\n",
      "5.93s\n",
      "1\n",
      "####################\n",
      "id : 47\n",
      "5.98s\n",
      "3\n",
      "####################\n",
      "id : 48\n",
      "5.98s\n",
      "3\n",
      "####################\n",
      "id : 49\n",
      "5.94s\n",
      "2\n",
      "####################\n",
      "##############################\n",
      "推論時間\n",
      "303.62s\n",
      "5.06m\n",
      "0.08h\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 推論を行うデータの確認、テストデータはコンペ期間中に追加される可能性があります、追加した際はslackにてアナウンスを行うのでご確認ください。\n",
    "test_data = json.load(open('new_test.json', 'r'))\n",
    "\n",
    "choice_data = choice(model, tokenizer, test_data)\n",
    "\n",
    "sec_choi = time.time() - start_time\n",
    "print(\"#\"*30)\n",
    "print('推論時間')\n",
    "print(f\"{sec_choi:.2f}s\")\n",
    "print(f\"{sec_choi/60:.2f}m\")\n",
    "print(f\"{sec_choi/60/60:.2f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXI_E20fu4aL"
   },
   "source": [
    "## 結果をマージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XBjRrgW_pJnL",
    "outputId": "9515bce9-508b-4145-8a04-748e0bf295a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "--------------------\n",
      "{'id': 0, 'task_type': 'multiple_choice', 'text': '物の外側をなす面を何と言うか？', 'choices': [{'choice_id': 1, 'text': '平面'}, {'choice_id': 2, 'text': '対面'}, {'choice_id': 3, 'text': '体'}, {'choice_id': 4, 'text': '表面'}, {'choice_id': 5, 'text': '顔面'}], 'answer': 4}\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_submission_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheck_submission_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_submission_data[\u001b[38;5;241m125\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "submit_data = json.load(open('submission.json', 'r'))\n",
    "\n",
    "for i in range(len(choice_data)):\n",
    "    if submit_data[i][\"task_type\"] == \"multiple_choice\":\n",
    "        submit_data[i][\"answer\"] = choice_data[i][\"answer\"]\n",
    "\n",
    "# 推論結果の保存、id, task_type, text, answerのkeyがあることを確認してください(正しく入っていない場合、スコア付けが行われません)\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(submit_data, f, indent=4)\n",
    "\n",
    "check_submission_data = json.load(open('submission.json', 'r'))\n",
    "print(len(check_submission_data))\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[0])\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[101])\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[125])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YdL5YlyNXRl0",
    "outputId": "591655dd-3129-4cc0-ae61-e996052e3977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Fri Oct 13 03:55:36 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   76C    P0              35W /  72W |  16505MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0              35W /  72W |  18029MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfqmvPRNpJnM"
   },
   "source": [
    "# ！メモリ解放のためカーネル再起動をお願いします！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D2b2ZP7sMqYE"
   },
   "outputs": [],
   "source": [
    "import sys, os, warnings, gc, time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mgQVes0ppJnM",
    "outputId": "52cf5528-e0c1-46e7-e646-48e8aac0e366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 13 03:56:50 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   70C    P8              20W /  72W |    123MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   70C    P8              19W /  72W |     17MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#演習環境\n",
    "# GPUの確認、枚数に応じaて割り当てられた計算資源の消費速度が異なるのでお気をつけください。コンペ期間中は2時間で自動でインスタンスが落ちず、無制限となるため、インスタンスの停止し忘れにもご注意ください。\n",
    "# 50GPU時間を超えると、インスタンスを立ち上げることができませんのでご注意ください。\n",
    "!nvidia-smi\n",
    "\n",
    "# /workspace/assets以下のファイルは永続化されます。作業中のcodeや学習した重みやデータはこちらに保存してください。(容量数TB、IOは遅い)\n",
    "# /workspace/assets以外の部分のデータ容量は100GBまでです。複数モデルをダウンロードしている場合は容量にご注意ください。\n",
    "# 具体的には以下のフォルダを適宜消す、あるいは/workspace/assets以下に移動するなどしてください。\n",
    "os.environ['HF_HOME'] = '/workspace/hf'\n",
    "\n",
    "!pip install -q \"optimum>=1.12.0\" openpyxl==3.1.2\n",
    "!pip install -q auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saiHdXBGpJnM"
   },
   "source": [
    "## モデル（openorca_stx）ダウンロード・読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "81721136b2a045948cfce66d9ea6584b",
      "57db9367b3324d34a5dfdaea13f7cebe",
      "ea097a95c5334e94b4ee2e9f9209146a",
      "025a65377cfa4aee959688205086eb8b",
      "a725df21acfc482eae919588ca0f744c",
      "f0ff6d3197e74d2e9607aed76815df54",
      "f6a8d00ea7bc4245a321c52d25aec5e1",
      "18982e8614ca4d49b1f6377b827a11cd",
      "f89cce4920044b2da42ef840a4e905df",
      "437f414d6718421a8b0a17b7f1fc5485",
      "cb986b0696224105a52ee30c997c3411",
      "4458acd4296943b88f0a42741c6d810e",
      "ec902c1c8ff8419cb9195dbcd6a76d31",
      "4da5c38a41f049f0816ad8f38d0ea266",
      "508b615e544347e3b450146e48a020f7",
      "5fc62bf03d0b46bcba6b45d6d6194d51",
      "2fa28b8ec2364dbc9b17709c1406f293",
      "e4290d0979e9491d9bca4246f87243dc",
      "ae52b1bad95c4ed7b30c4dd425215a72",
      "4791e201e66449f5a66894ff949f918d",
      "81a5556a5f14478e85d819c36028bfb6",
      "533da1f450e34e769c410bf9f829d589",
      "de0425074afb435788e2a77e6191c620",
      "7a992bfd7957432ea6bde37157b2f913",
      "eabefbbc59ee458f9690d0318d09f77a",
      "1d8b64d06a7c414e873f3588e0049dcc",
      "a1be925610044bf2b5272a2066a05932",
      "733f854315474b3bbd576c5157718cb7",
      "acac753007be435f84e538ec880f0684",
      "4fa5b6d961f64f74b85c3ad6103d4885",
      "ab30dd5214964c4f955882b3fd75479b",
      "326ba2bfaf4a4efdb8857f28acfa1063",
      "c0168e0c7a7444308449f48f50200f19",
      "d68f0253b61c4d37909ccdd6a5c959d8",
      "6641741aac194e8193a8ed1624fc1934",
      "7b5ee38f288f4915bba17af4d5244c45",
      "3523beb751374638ae43f7d4d33ba00c",
      "af46a1bb2d16416f906b5b20b2fc9bfe",
      "1f2cc73940824c41b4131fa8e1253091",
      "5a7cb6cf9e884acab4cb9419b5d97658"
     ]
    },
    "id": "9cUOpQ1ipJnN",
    "outputId": "a74d551f-728a-4ddc-dc68-e2e1c49a06b8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d132cc0fbec4eeaacecf2cdf38f0cae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a4632f55e847dfbf09152c5bb710b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-13 03:57:16,738] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e0f55b024345e9979b2488b0125730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/29.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1caadcacff4712a6d2490fa196a69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab790308d8c9470d88cdf4f3fa2efb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f6865e87a745bfbfcb3de18df01495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a790f4ae1154be7993a59153c12296f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ea7f42a480448d94b43cd72c85d73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41544cd650804d6984e47eabb7bea74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/158 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "ロード時間\n",
      "113.17s\n",
      "1.89m\n",
      "0.03h\n"
     ]
    }
   ],
   "source": [
    "# モデルのダウンロードと読み込み\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed, logging\n",
    "import torch\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_name = \"lightblue/openorca_stx\"\n",
    "\n",
    "tokenizer_name = model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "use_fast=False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=use_fast)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map='auto',\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    #use_safetensors=True,\n",
    "    #revision=revision,\n",
    "    )\n",
    "\n",
    "sec_load2 = time.time() - start_time\n",
    "print(\"#\"*30)\n",
    "print('ロード時間')\n",
    "print(f\"{sec_load2:.2f}s\")\n",
    "print(f\"{sec_load2/60:.2f}m\")\n",
    "print(f\"{sec_load2/60/60:.2f}h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fVHgmTbgpJnN",
    "outputId": "3f67fdff-584c-4dc9-cc1e-7a515bec0508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 13 03:58:58 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
      "| N/A   68C    P0              32W /  72W |   3453MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   66C    P0              31W /  72W |   4321MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfGfMCmipJnN",
    "outputId": "3e906dfb-987e-4049-92c2-a947a8562302"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 0,\n",
       " 'model.layers.4': 0,\n",
       " 'model.layers.5': 0,\n",
       " 'model.layers.6': 0,\n",
       " 'model.layers.7': 0,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 0,\n",
       " 'model.layers.10': 0,\n",
       " 'model.layers.11': 0,\n",
       " 'model.layers.12': 0,\n",
       " 'model.layers.13': 0,\n",
       " 'model.layers.14': 0,\n",
       " 'model.layers.15': 0,\n",
       " 'model.layers.16': 0,\n",
       " 'model.layers.17': 1,\n",
       " 'model.layers.18': 1,\n",
       " 'model.layers.19': 1,\n",
       " 'model.layers.20': 1,\n",
       " 'model.layers.21': 1,\n",
       " 'model.layers.22': 1,\n",
       " 'model.layers.23': 1,\n",
       " 'model.layers.24': 1,\n",
       " 'model.layers.25': 1,\n",
       " 'model.layers.26': 1,\n",
       " 'model.layers.27': 1,\n",
       " 'model.layers.28': 1,\n",
       " 'model.layers.29': 1,\n",
       " 'model.layers.30': 1,\n",
       " 'model.layers.31': 1,\n",
       " 'model.layers.32': 1,\n",
       " 'model.layers.33': 1,\n",
       " 'model.layers.34': 1,\n",
       " 'model.layers.35': 1,\n",
       " 'model.layers.36': 1,\n",
       " 'model.layers.37': 1,\n",
       " 'model.layers.38': 1,\n",
       " 'model.layers.39': 1,\n",
       " 'model.norm': 1,\n",
       " 'lm_head': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paCJY8MOpJnO",
    "outputId": "469d335e-1ab0-4e4c-cb6d-ec59b4f6975b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32002, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32002, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqkFn277z5eP"
   },
   "source": [
    "## 推論用関数（要約タスク）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2iMfzpd7Xr77"
   },
   "outputs": [],
   "source": [
    "def summarization(model, tokenizer, test_data):\n",
    "\n",
    "    count = 0\n",
    "    for data in test_data:\n",
    "        count += 1\n",
    "\n",
    "        if data['task_type'] == 'summarization':\n",
    "            print(\"id :\",count-1)\n",
    "            max_length = 2048+256\n",
    "            text = f\"\"\"{data['text']}\\n\\nあなたは要約のスペシャリストです。上記の日本語の文章を理解して日本語で要約してください。句点で文章が終わり、要約以外の文章（注釈や参照および備考）は不要です。\\n\\n要約:\"\"\"\n",
    "            token_ids = tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "            max_new_tokens = int(max(min(max_length, len(token_ids[0]))*0.1, 128))\n",
    "            min_new_tokens = int(max(min(max_length, len(token_ids[0]))*0.025, 20))\n",
    "            token_ids = token_ids[:, -(max_length):]\n",
    "            token_ids_size = len(token_ids[0])\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output_ids = model.generate(\n",
    "                    inputs=token_ids.to(model.device),\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    min_new_tokens=min_new_tokens,\n",
    "                    no_repeat_ngram_size=10,\n",
    "                    max_time=60*5,\n",
    "                )\n",
    "\n",
    "            print(f\"{time.time() - start_time:.2f}s\")\n",
    "            del token_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            output = tokenizer.decode(output_ids.tolist()[0][token_ids_size:], skip_special_tokens=True)\n",
    "            del output_ids\n",
    "            torch.cuda.empty_cache()\n",
    "            print(output)\n",
    "            print(\"#\"*20)\n",
    "        else:\n",
    "            output=\"\"\n",
    "\n",
    "        data['answer'] = output\n",
    "        del output\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmtmQuGIpJnO"
   },
   "source": [
    "## 要約タスクの推論(Type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VGnEwxp5pJnP",
    "outputId": "285d017f-12a2-429b-dc6e-f4390b489c95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 50\n",
      "256.80s\n",
      "中国は13日、アメリカに対して対中関税を引き上げると発表した。アメリカが10日に中国からの輸入に対して25％の関税を追設したことを受けたもの。\n",
      "####################\n",
      "id : 51\n",
      "233.36s\n",
      "ネパールの世界遺産ルンビニは、大気汚染によって歴史的価値が傷つけられている。国際自然保護連合（IUCN）が調査した。\n",
      "####################\n",
      "id : 52\n",
      "202.01s\n",
      "英国の科学者たちは、英国の欧州連合（EU）離脱（ブレグジット）をめぐり、英国政府とEUとの間で科学分野における合意を求めている。\n",
      "####################\n",
      "id : 53\n",
      "224.86s\n",
      "米テキサス州で25日から続く熱帯低圧「ハービー」による大雨で、同州の州都ヒューストンでは28日までに2000人以上が救助された。\n",
      "####################\n",
      "id : 54\n",
      "154.72s\n",
      "スリランカで21日、イースター（復命祭）の日にカトリック教会やホステルなどが狙われ、少なくとも207人が死亡し、約500人が負傷した。\n",
      "####################\n",
      "id : 55\n",
      "117.62s\n",
      "米ニューヨーク市のマイケル・ブルームベルグ前市長が、2016年米大統領選に独立候補となる可能性を検討していたことが明らかになった。\n",
      "####################\n",
      "id : 56\n",
      "164.21s\n",
      "インドネシア・スマットラ島バタンガンサールで9月30日、村の警備員がニシキヘビと格戦し、勝利した。村の人たちはぬいぐるみのように巨大なヘビを吊るし、油で揚げ、みんなで食べた。\n",
      "####################\n",
      "id : 57\n",
      "126.72s\n",
      "ナイジェリア北部でイスラム教徒の武装組織「ボコ・ハラム」に拉致された少女たちの映像が公開さ され、政府は少女たちの解放に向かう努力を続けていると述べた。\n",
      "####################\n",
      "id : 58\n",
      "175.43s\n",
      "米大統領選の共和党候補指名をめぐり、実業家ドナルド・トランプ前大統領候補が26日、必要な代議哀数を獲得したと発表した。\n",
      "####################\n",
      "id : 59\n",
      "242.54s\n",
      "欧州連合（EU）は16日、トルコとの間で移民協定について合意したと発表した。EUはトルコに対して、シリア人1人が送られるごとにトルコに滞在するシリア人1人が旅行できるようにするという取り決えを含んでいる。\n",
      "####################\n",
      "id : 60\n",
      "268.17s\n",
      "米大統領選の民主党予兆選で、ヒラリー・クリントン氏が勝利した。ニューヨーク州では、バーニー・サンダース氏が勝ったが、残る予傘選では勝てないとみられている。\n",
      "####################\n",
      "id : 61\n",
      "71.83s\n",
      "100%オレンジのジュースは多くのビタミンCを含むため、健康に良いとされております。しかし同時に糖分も多いため、飲み違うと注意しなければなり、オレンジの食物繊維は取り除かれている点も注意です。\n",
      "####################\n",
      "id : 62\n",
      "83.42s\n",
      "日本の宇宙航空開発機構（JAXA)は12月14日、小惑星「はやぶさ2」からのサンプルを分析したところ、リュウグウ由来の砂粒状の砂が採取されたと発表した。\n",
      "####################\n",
      "id : 63\n",
      "82.07s\n",
      "外食チェーン大手の「ハイデイ日高」は6日、2023年2～8月期の営業損矩が24億2000万円の黒字になり、前年同期（赤字）から大幅回復したと発表。\n",
      "####################\n",
      "id : 64\n",
      "74.25s\n",
      "友達と遊びに行った。上野駅で待ち合いして、スターバックスに行き喋りました。その後、寿司屋に行き回らない寿司を選んで食べました。最後に、上野動物園に行き、ゾウ、ライオンなどがいます。特にパンダがかわいいです。\n",
      "####################\n",
      "id : 65\n",
      "83.92s\n",
      "資本主義は、効率性を高くする特性を持っているが、不平等や環境への影力、不確実性などの欠点もある。\n",
      "####################\n",
      "id : 66\n",
      "77.06s\n",
      "2022年にChatGTPが登場してから、さまざまな大胆言語モデルや、それらを使ったサービスが続々と登場している。\n",
      "####################\n",
      "id : 67\n",
      "67.48s\n",
      "エスノメソドロジーは、人々が何を志向してどのよりう物事をおこない、それらを結びつけているということを研究する。\n",
      "####################\n",
      "id : 68\n",
      "37.07s\n",
      "大手菓子メールの「明治」は、執行役員の杉浦敏郎氏が明らかにした。\n",
      "####################\n",
      "id : 69\n",
      "83.84s\n",
      "今日、最終試験の課題として「テストデータの作 成」が出された。特に提出は必項とはなっておらず、 希望者のみの作成と なっているが、作成 する必要のあるタス クは次の3つである\n",
      "####################\n",
      "##############################\n",
      "推論時間\n",
      "2829.08s\n",
      "47.15m\n",
      "0.79h\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import json\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 推論を行うデータの確認、テストデータはコンペ期間中に追加される可能性があります、追加した際はslackにてアナウンスを行うのでご確認ください。\n",
    "test_data = json.load(open('new_test.json', 'r'))\n",
    "\n",
    "summarization_data = summarization(model, tokenizer, test_data)\n",
    "\n",
    "sec_summ = time.time() - start_time\n",
    "print(\"#\"*30)\n",
    "print('推論時間')\n",
    "print(f\"{sec_summ:.2f}s\")\n",
    "print(f\"{sec_summ/60:.2f}m\")\n",
    "print(f\"{sec_summ/60/60:.2f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfylemhOpJnm"
   },
   "source": [
    "## 結果をマージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmF0DCNupJnm",
    "outputId": "1ab20821-a415-4997-e5fa-62df67de4818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "--------------------\n",
      "{'id': 0, 'task_type': 'multiple_choice', 'text': '物の外側をなす面を何と言うか？', 'choices': [{'choice_id': 1, 'text': '平面'}, {'choice_id': 2, 'text': '対面'}, {'choice_id': 3, 'text': '体'}, {'choice_id': 4, 'text': '表面'}, {'choice_id': 5, 'text': '顔面'}], 'answer': 4}\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_submission_data[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcheck_submission_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m101\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(check_submission_data[\u001b[38;5;241m125\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "submit_data = json.load(open('submission.json', 'r'))\n",
    "\n",
    "for i in range(len(summarization_data)):\n",
    "    if submit_data[i][\"task_type\"] == \"summarization\":\n",
    "        submit_data[i][\"answer\"] = summarization_data[i][\"answer\"]\n",
    "\n",
    "# 推論結果の保存、id, task_type, text, answerのkeyがあることを確認してください(正しく入っていない場合、スコア付けが行われません)\n",
    "with open('submission.json', 'w') as f:\n",
    "    json.dump(submit_data, f, indent=4)\n",
    "\n",
    "check_submission_data = json.load(open('submission.json', 'r'))\n",
    "print(len(check_submission_data))\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[0])\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[101])\n",
    "print(\"-\"*20)\n",
    "print(check_submission_data[125])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
